# GuÃ­a Completa: Pipeline Serverless ETL

## ğŸ“Œ Resumen Ejecutivo

Esta guÃ­a documenta el sistema completo de ETL (Extract, Transform, Load) 100% serverless para el proyecto de Unidades de Proyecto de la AlcaldÃ­a de Cali.

### Flujo Completo

```
1. EXTRACCIÃ“N (Local)
   Google Sheets â†’ Python script â†’ GeoJSON raw

2. TRANSFORMACIÃ“N (Local)
   GeoJSON raw â†’ Transformaciones â†’ S3 Bucket

3. CARGA (Serverless)
   S3 Bucket â†’ Cloud Functions â†’ Firestore
```

## ğŸ¯ Objetivos Cumplidos

âœ… Upload automÃ¡tico a S3 despuÃ©s de transformaciÃ³n  
âœ… Arquitectura 100% serverless con Cloud Functions  
âœ… Upsert inteligente (solo actualiza cambios)  
âœ… Credenciales AWS seguras en Secret Manager  
âœ… Trigger manual y automÃ¡tico (Cloud Scheduler)  
âœ… Mapeo correcto de campos segÃºn especificaciones  
âœ… Colecciones separadas para datos, logs y reportes  
âœ… Respeta estructura de Firestore existente

## ğŸ—ï¸ Arquitectura Final

### Componentes

1. **ExtracciÃ³n (Local Python)**

   - `extraction_app/data_extraction_unidades_proyecto.py`
   - Lee desde Google Sheets
   - Genera GeoJSON crudo

2. **TransformaciÃ³n (Local Python)**

   - `transformation_app/data_transformation_unidades_proyecto.py`
   - Limpieza, validaciÃ³n, geocodificaciÃ³n
   - Upload automÃ¡tico a S3 vÃ­a `utils/s3_uploader.py`

3. **S3 Storage (AWS)**

   - Bucket: `unidades-proyecto-documents`
   - Folders:
     - `/up-geodata/` â†’ Datos transformados
     - `/logs/` â†’ Logs de transformaciÃ³n
     - `/reports/` â†’ Reportes de calidad

4. **Cloud Functions (GCP)**

   - `load-unidades-proyecto` â†’ Carga principal
   - `manual-trigger-unidades-proyecto` â†’ Trigger manual
   - Lee de S3, escribe a Firestore con upsert

5. **Firestore (GCP)**
   - `unidades_proyecto` â†’ Datos principales
   - `unidades_proyecto_transformation_logs` â†’ Logs
   - `unidades_proyecto_transformation_reports` â†’ Reportes

### Diagrama de Flujo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Google Sheets   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ extraction_app/
         â”‚ data_extraction_unidades_proyecto.py
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GeoJSON Raw     â”‚
â”‚ (local file)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ transformation_app/
         â”‚ data_transformation_unidades_proyecto.py
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformations â”‚
â”‚ â€¢ Geocoding     â”‚
â”‚ â€¢ Validation    â”‚
â”‚ â€¢ Intersections â”‚
â”‚ â€¢ Normalization â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ utils/s3_uploader.py
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S3 Bucket                       â”‚
â”‚ unidades-proyecto-documents     â”‚
â”‚                                 â”‚
â”‚ /up-geodata/                    â”‚
â”‚   â””â”€â”€ unidades_proyecto_        â”‚
â”‚       transformed.geojson       â”‚
â”‚                                 â”‚
â”‚ /logs/                          â”‚
â”‚   â””â”€â”€ transformation_log_*.json â”‚
â”‚                                 â”‚
â”‚ /reports/                       â”‚
â”‚   â””â”€â”€ transformation_report_*.  â”‚
â”‚       json                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ cloud_functions/main.py
         â”‚ â€¢ Reads from S3
         â”‚ â€¢ AWS creds from Secret Manager
         â”‚ â€¢ MD5 hash comparison
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloud Functions (GCP)           â”‚
â”‚                                 â”‚
â”‚ Trigger Options:                â”‚
â”‚ â€¢ Manual (HTTP POST)            â”‚
â”‚ â€¢ Scheduler (diario 2 AM)       â”‚
â”‚ â€¢ Pipeline (env var)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Intelligent Upsert
         â”‚ (only changes)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Firestore Collections           â”‚
â”‚                                 â”‚
â”‚ â€¢ unidades_proyecto             â”‚
â”‚   â””â”€â”€ 1,641 documents           â”‚
â”‚                                 â”‚
â”‚ â€¢ unidades_proyecto_            â”‚
â”‚   transformation_logs           â”‚
â”‚                                 â”‚
â”‚ â€¢ unidades_proyecto_            â”‚
â”‚   transformation_reports        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Setup Paso a Paso

### Paso 1: Configurar AWS S3

```powershell
# 1.1. Configurar credenciales AWS
cd a:\programing_workspace\proyectos_cali_alcaldia_etl
.\setup_aws_quick.ps1

# Esto crea: aws_credentials.json con credenciales permanentes

# 1.2. Verificar bucket
aws s3 ls s3://unidades-proyecto-documents/
```

**Resultado esperado:**

```
âœ“ Credenciales AWS configuradas
âœ“ Bucket: unidades-proyecto-documents
âœ“ Usuario: juanpgm (706341499736)
```

### Paso 2: Ejecutar TransformaciÃ³n con Upload a S3

```powershell
# 2.1. Activar entorno virtual
.\env\Scripts\Activate.ps1

# 2.2. Ejecutar transformaciÃ³n
python transformation_app\data_transformation_unidades_proyecto.py
```

**Resultado esperado:**

```
TRANSFORMATION PIPELINE COMPLETED
âœ“ Processed data: 1,641 records
âœ“ Total columns: 65
âœ“ GeoDataFrame type: GeoDataFrame

UPLOADING OUTPUTS TO S3
âœ“ Uploaded: up-geodata/unidades_proyecto_transformed.geojson
âœ“ Uploaded: logs/transformation_log_20240101_120000.json
âœ“ Uploaded: reports/transformation_report_20240101_120000.json

S3 UPLOAD COMPLETED
```

### Paso 3: Configurar Google Cloud Functions

```powershell
# 3.1. Cambiar a directorio de Cloud Functions
cd cloud_functions

# 3.2. Ejecutar setup (necesita Project ID de GCP)
.\setup_cloud_functions.ps1 -ProjectId "tu-proyecto-gcp-123"
```

**El script automÃ¡ticamente:**

1. âœ… Habilita APIs de GCP
2. âœ… Crea Secret Manager con credenciales AWS
3. âœ… Configura Service Account
4. âœ… Despliega Cloud Functions
5. âœ… Configura Cloud Scheduler (opcional)

**Resultado esperado:**

```
============================================
   CONFIGURACIÃ“N COMPLETADA
============================================

ğŸ“‹ URLs de las funciones:
  Principal: https://us-central1-tu-proyecto.cloudfunctions.net/load-unidades-proyecto
  Manual: https://us-central1-tu-proyecto.cloudfunctions.net/manual-trigger-unidades-proyecto

ğŸš€ Para ejecutar manualmente:
  Invoke-WebRequest -Uri 'https://...' -Method POST

âœ… Setup completado exitosamente!
```

### Paso 4: Probar Cloud Function Manualmente

```powershell
# 4.1. Ejecutar trigger manual
$url = "https://us-central1-tu-proyecto.cloudfunctions.net/manual-trigger-unidades-proyecto"
Invoke-WebRequest -Uri $url -Method POST

# 4.2. Ver resultado
# Response deberÃ­a ser 200 OK con JSON:
{
  "success": true,
  "stats": {
    "unidades_proyecto": {
      "new": 1641,
      "updated": 0,
      "unchanged": 0
    },
    "total_processed": 1641
  }
}
```

### Paso 5: Verificar en Firestore

```powershell
# 5.1. Ver colecciones
gcloud firestore collections list

# Esperado:
# unidades_proyecto
# unidades_proyecto_transformation_logs
# unidades_proyecto_transformation_reports

# 5.2. Contar documentos (aproximado)
gcloud firestore export gs://tu-bucket-backup/ --collection-ids=unidades_proyecto
```

**O en Firestore Console:**

- https://console.firebase.google.com/project/TU_PROYECTO/firestore

Verificar:

- âœ… ColecciÃ³n `unidades_proyecto` tiene ~1,641 documentos
- âœ… Campo `upid` es el identificador Ãºnico
- âœ… Campos `comuna_corregimiento`, `barrio_vereda` usan valores `_2` cuando corresponde
- âœ… Fechas son `fecha_inicio` y `fecha_fin` (no `_std`)

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Trigger AutomÃ¡tico desde Pipeline

Para que la transformaciÃ³n automÃ¡ticamente ejecute la Cloud Function:

```powershell
# Configurar variables de entorno
$env:TRIGGER_CLOUD_FUNCTION = "true"
$env:CLOUD_FUNCTION_URL = "https://us-central1-tu-proyecto.cloudfunctions.net/load-unidades-proyecto"

# Ejecutar transformaciÃ³n
python transformation_app\data_transformation_unidades_proyecto.py
```

**Resultado esperado:**

```
S3 UPLOAD COMPLETED

TRIGGERING CLOUD FUNCTION - FIRESTORE LOAD

âœ… Cloud Function ejecutada exitosamente
  â€¢ Documentos nuevos: 45
  â€¢ Documentos actualizados: 128
  â€¢ Documentos sin cambios: 1468
  â€¢ Logs cargados: 1
  â€¢ Reportes cargados: 1
```

### Cloud Scheduler (EjecuciÃ³n Diaria)

Si configuraste Cloud Scheduler durante el setup:

```powershell
# Ver jobs
gcloud scheduler jobs list --location=us-central1

# Ejecutar manualmente
gcloud scheduler jobs run etl-unidades-proyecto-daily --location=us-central1

# Pausar
gcloud scheduler jobs pause etl-unidades-proyecto-daily --location=us-central1

# Reanudar
gcloud scheduler jobs resume etl-unidades-proyecto-daily --location=us-central1

# Cambiar horario
gcloud scheduler jobs update http etl-unidades-proyecto-daily \
  --location=us-central1 \
  --schedule="0 3 * * *"  # 3 AM en vez de 2 AM
```

## ğŸ“Š Mapeo de Campos (EspecificaciÃ³n Cumplida)

### Campo: `comuna_corregimiento`

```python
if fuera_rango == 'ACEPTABLE':
    comuna_corregimiento = comuna_corregimiento_2
else:  # 'FUERA DE RANGO'
    comuna_corregimiento = comuna_corregimiento  # original
```

### Campo: `barrio_vereda`

```python
# Prioridad: barrio_vereda_2 si es vÃ¡lido, sino barrio_vereda original
barrio_vereda = barrio_vereda_2 if barrio_vereda_2 else barrio_vereda
```

### Campos de Fecha

```python
fecha_inicio = fecha_inicio_std  # Renombrado
fecha_fin = fecha_fin_std        # Renombrado
# Los campos _std NO se guardan en Firestore
```

### Identificador Ãšnico

```python
document_id = upid  # Campo 'upid' es el ID Ãºnico del documento
```

## ğŸ” Monitoreo y Logs

### Ver Logs de Cloud Functions

```powershell
# Ãšltimos 50 logs
gcloud functions logs read load-unidades-proyecto --region=us-central1 --limit=50

# Filtrar errores
gcloud functions logs read load-unidades-proyecto --region=us-central1 | Select-String "ERROR"

# Logs en tiempo real
gcloud functions logs read load-unidades-proyecto --region=us-central1 --follow
```

### MÃ©tricas en GCP Console

1. Ir a: https://console.cloud.google.com/functions
2. Seleccionar: `load-unidades-proyecto`
3. Tab: **Metrics**

Ver:

- Invocaciones por dÃ­a
- Tiempo de ejecuciÃ³n promedio
- Errores
- Uso de memoria

### Verificar S3 Upload

```powershell
# Listar archivos en S3
aws s3 ls s3://unidades-proyecto-documents/up-geodata/
aws s3 ls s3://unidades-proyecto-documents/logs/
aws s3 ls s3://unidades-proyecto-documents/reports/

# Descargar archivo para verificar
aws s3 cp s3://unidades-proyecto-documents/up-geodata/unidades_proyecto_transformed.geojson ./test.geojson
```

## ğŸ› Troubleshooting

### Error: "Secret not found"

**SÃ­ntoma:**

```
ERROR: Secret projects/.../secrets/aws-credentials not found
```

**SoluciÃ³n:**

```powershell
# Re-ejecutar setup
cd cloud_functions
.\setup_cloud_functions.ps1
```

### Error: "Permission denied" en Firestore

**SÃ­ntoma:**

```
403 Permission denied on Firestore
```

**SoluciÃ³n:**

```powershell
# Verificar Service Account
gcloud projects get-iam-policy tu-proyecto-gcp \
  --flatten="bindings[].members" \
  --filter="bindings.members:serviceAccount:cloud-functions-etl@*"

# Re-asignar permisos
gcloud projects add-iam-policy-binding tu-proyecto-gcp \
  --member="serviceAccount:cloud-functions-etl@tu-proyecto-gcp.iam.gserviceaccount.com" \
  --role="roles/datastore.user"
```

### Error: "Cannot connect to S3"

**SÃ­ntoma:**

```
ERROR: Could not connect to S3 bucket
```

**SoluciÃ³n:**

```powershell
# 1. Verificar credenciales AWS localmente
aws sts get-caller-identity

# 2. Verificar Secret Manager tiene las credenciales
gcloud secrets versions access latest --secret="aws-credentials"

# 3. Re-crear secret si es necesario
cd cloud_functions
.\setup_cloud_functions.ps1
```

### Error: Timeout en Cloud Function

**SÃ­ntoma:**

```
Function execution timed out after 60s
```

**SoluciÃ³n:**

```powershell
# Aumentar timeout
gcloud functions deploy load-unidades-proyecto \
  --timeout=540s \
  --memory=1GB \
  --gen2 \
  --region=us-central1 \
  --source=./cloud_functions \
  --entry-point=load_unidades_proyecto_from_s3
```

## ğŸ“ˆ Costos Estimados (GCP)

### Cloud Functions (Gen 2)

- **Invocaciones:** Primeros 2M gratis/mes
- **Compute:**
  - 512MB RAM, ~30s ejecuciÃ³n
  - ~$0.0000025 por invocaciÃ³n
  - ~$0.075/mes (1 ejecuciÃ³n diaria)

### Secret Manager

- **Versiones activas:** Primeros 6 gratis
- **Accesos:** Primeros 10K gratis/mes
- ~$0.00/mes

### Firestore

- **Lecturas:** Primeros 50K gratis/dÃ­a
- **Escrituras:** Primeros 20K gratis/dÃ­a
- **Storage:** Primero 1GB gratis
- ~$0.00/mes (uso normal)

### Cloud Scheduler

- **Jobs:** Primeros 3 gratis
- ~$0.00/mes

**Total estimado:** <$1/mes

## âœ… Checklist de ValidaciÃ³n

### Fase 1: S3 Setup

- [ ] `aws_credentials.json` existe y tiene credenciales vÃ¡lidas
- [ ] `aws s3 ls s3://unidades-proyecto-documents/` funciona
- [ ] Bucket tiene folders: `/up-geodata/`, `/logs/`, `/reports/`

### Fase 2: TransformaciÃ³n + Upload

- [ ] `python transformation_app\data_transformation_unidades_proyecto.py` ejecuta sin errores
- [ ] Output muestra "S3 UPLOAD COMPLETED"
- [ ] `aws s3 ls s3://unidades-proyecto-documents/up-geodata/` muestra `unidades_proyecto_transformed.geojson`

### Fase 3: Cloud Functions Setup

- [ ] `.\setup_cloud_functions.ps1` completa sin errores
- [ ] Obtienes 2 URLs de Cloud Functions
- [ ] `gcloud secrets list` muestra `aws-credentials`
- [ ] `gcloud functions list` muestra `load-unidades-proyecto` y `manual-trigger-unidades-proyecto`

### Fase 4: EjecuciÃ³n y ValidaciÃ³n

- [ ] `Invoke-WebRequest -Uri $url -Method POST` retorna 200 OK
- [ ] Response JSON muestra `"success": true`
- [ ] Firestore Console muestra colecciÃ³n `unidades_proyecto` con documentos
- [ ] Documentos tienen campos correctos: `upid`, `comuna_corregimiento`, `barrio_vereda`, `fecha_inicio`, `fecha_fin`

### Fase 5: AutomatizaciÃ³n (Opcional)

- [ ] Cloud Scheduler job configurado
- [ ] `gcloud scheduler jobs list` muestra `etl-unidades-proyecto-daily`
- [ ] Pipeline con `TRIGGER_CLOUD_FUNCTION=true` ejecuta Cloud Function automÃ¡ticamente

## ğŸ“ Contacto y Soporte

- **Logs:** GCP Console â†’ Cloud Functions â†’ Logs
- **Errores S3:** Verificar `aws_credentials.json` y permisos IAM
- **Errores GCP:** Verificar Service Account y permisos Firestore
- **Performance:** Ajustar `--memory` y `--timeout` en deploy

## ğŸ“ PrÃ³ximos Pasos

1. âœ… **Completado:** Setup completo de pipeline serverless
2. â¬œ **Opcional:** Configurar alertas en Cloud Monitoring
3. â¬œ **Opcional:** Implementar backup automÃ¡tico de Firestore
4. â¬œ **Opcional:** Dashboard de mÃ©tricas en Looker Studio
5. â¬œ **Opcional:** CI/CD con GitHub Actions para deploy automÃ¡tico

---

**Ãšltima actualizaciÃ³n:** 2024  
**VersiÃ³n:** 1.0 - Serverless Pipeline Completo
