{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1366d4f6",
   "metadata": {},
   "source": [
    "# RECREANDO BASE DE DATOS PERMANENTE DE UNIDADES DE PROYECTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use absolute path based on notebook location\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "file_path = os.path.join(notebook_dir, \"basemaps\", \"UPS 14 FEB 26.xlsx\")\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(file_path):\n",
    "\tprint(f\"File not found: {file_path}\")\n",
    "\tprint(f\"Current working directory: {os.getcwd()}\")\n",
    "\t# Try alternative path (one level up)\n",
    "\tfile_path = os.path.join(os.path.dirname(notebook_dir), \"basemaps\", \"UPS 14 FEB 26.xlsx\")\n",
    "\t\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa4e4a",
   "metadata": {},
   "source": [
    "## NORMALIZACIÓN nombre_up Y nombre_up_detalle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae18467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reserved words that should always be uppercase\n",
    "reserved_words = ['I.E','I.E.', 'UTC', 'CALI', 'C.A.L.I', 'CAM', 'IPS', 'UNP', 'UP', 'SENA', 'S.A.S', 'S.A.S.', 'E.S.E', 'E.S.E.', 'C.A.L.I.', 'II']\n",
    "\n",
    "def capitalize_with_exceptions(text):\n",
    "    \"\"\"Capitalize first letter of each word, keeping reserved words uppercase\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    words = str(text).split()\n",
    "    result = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Check if word is a reserved word (case-insensitive)\n",
    "        if any(word.upper() == res_word for res_word in reserved_words):\n",
    "            result.append(word.upper())\n",
    "        else:\n",
    "            # Capitalize only first letter, rest lowercase\n",
    "            result.append(word.capitalize())\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "# Apply the function to both columns\n",
    "df['nombre_up'] = df['nombre_up'].apply(capitalize_with_exceptions)\n",
    "df['nombre_up_detalle'] = df['nombre_up_detalle'].apply(capitalize_with_exceptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7e85e",
   "metadata": {},
   "source": [
    "## UNIFICACIÓN DE COORDENADAS EN UN PAR ÚNICO \"coordinates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c2d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'coordinates' with format \"lon, lat\"\n",
    "df['coordinates'] = df['lon'].astype(str) + ', ' + df['lat'].astype(str)\n",
    "\n",
    "# Drop the original lat and lon columns\n",
    "df.drop(columns=['lat', 'lon'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56567575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df4a95",
   "metadata": {},
   "source": [
    "## CREACION DE UPID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UPIDs with special handling for Vivienda Social y Habitat\n",
    "df = df.copy()\n",
    "\n",
    "# Define vivienda_mask to identify Vivienda Social y Habitat records\n",
    "vivienda_mask = (df['nombre_centro_gestor'] == 'Secretaría de Vivienda Social y Habitat') & (\n",
    "    df['nombre_up'].str.contains('Vivienda', case=False, na=False)\n",
    ")\n",
    "\n",
    "# Non-vivienda: group by nombre_up and nombre_up_detalle (start at 1)\n",
    "non_vivienda = df[~vivienda_mask]\n",
    "group_ids = non_vivienda.groupby(['nombre_up', 'nombre_up_detalle'], dropna=False).ngroup() + 1\n",
    "group_ids = group_ids.astype('Int64')\n",
    "\n",
    "# Vivienda: unique UPID per record (start at 1)\n",
    "vivienda = df[vivienda_mask]\n",
    "vivienda_ids = pd.Series(range(1, len(vivienda) + 1), index=vivienda.index)\n",
    "\n",
    "# Offset vivienda IDs to avoid collisions\n",
    "offset = int(group_ids.max()) if not group_ids.empty else 0\n",
    "df.loc[~vivienda_mask, 'upid'] = 'UNP-' + group_ids.astype(str)\n",
    "df.loc[vivienda_mask, 'upid'] = 'UNP-' + (vivienda_ids + offset).astype('Int64').astype(str)\n",
    "\n",
    "# Reorder columns to put upid first\n",
    "cols = ['upid'] + [col for col in df.columns if col != 'upid']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f544b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e96ca0a",
   "metadata": {},
   "source": [
    "## RECONOCIMIENTO UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6d93c",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Crear df_up con las columnas solicitadas\n",
    "df_up = df[['upid', 'nombre_up', 'nombre_up_detalle', 'direccion', 'tipo_equipamiento', 'coordinates']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aae961",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e50c6c4",
   "metadata": {},
   "source": [
    "### OBTENCIÓN DE \"comuna_corregimiento\" y \"barrio_vereda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bab3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the geojson layers (go up one level from pipelines folder to project root)\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "comunas_gdf = gpd.read_file(os.path.join(project_root, \"basemaps\", \"comunas_corregimientos.geojson\"))\n",
    "barrios_gdf = gpd.read_file(os.path.join(project_root, \"basemaps\", \"barrios_veredas.geojson\"))\n",
    "\n",
    "# Convert coordinates string to Point geometry\n",
    "def coords_to_point(coords_str):\n",
    "    \"\"\"Convert 'lon, lat' string to Point geometry\"\"\"\n",
    "    try:\n",
    "        lon, lat = map(float, coords_str.split(', '))\n",
    "        return Point(lon, lat)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Create temporary GeoDataFrame from df_up\n",
    "df_up_geo = df_up.copy()\n",
    "df_up_geo['geometry'] = df_up_geo['coordinates'].apply(coords_to_point)\n",
    "df_up_geo = gpd.GeoDataFrame(df_up_geo, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Spatial join with comunas_corregimientos layer\n",
    "df_up_geo = gpd.sjoin(df_up_geo, comunas_gdf[['geometry', 'comuna_corregimiento']], how='left', predicate='within')\n",
    "df_up['comuna_corregimiento'] = df_up_geo['comuna_corregimiento']\n",
    "\n",
    "# Spatial join with barrios_veredas layer\n",
    "df_up_geo = gpd.sjoin(df_up_geo[['nombre_up', 'nombre_up_detalle', 'direccion', 'tipo_equipamiento', 'coordinates', 'geometry']], \n",
    "                       barrios_gdf[['geometry', 'barrio_vereda']], how='left', predicate='within')\n",
    "df_up['barrio_vereda'] = df_up_geo['barrio_vereda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert coordinates string to GeoJSON-compliant geometry\n",
    "def coords_to_geojson_geometry(coords_str):\n",
    "    \"\"\"Convert 'lon, lat' string to RFC 7946 compliant GeoJSON Point geometry\"\"\"\n",
    "    if pd.isna(coords_str):\n",
    "        return None\n",
    "    try:\n",
    "        lon, lat = map(float, coords_str.split(', '))\n",
    "        # RFC 7946 GeoJSON Point format\n",
    "        return {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [lon, lat]\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Replace coordinates column with geometry column\n",
    "df_up['geometry'] = df_up['coordinates'].apply(coords_to_geojson_geometry)\n",
    "df_up = df_up.drop(columns=['coordinates'])\n",
    "\n",
    "# Reorder columns to put geometry at the end (common convention)\n",
    "cols = [col for col in df_up.columns if col != 'geometry'] + ['geometry']\n",
    "df_up = df_up[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a344ab",
   "metadata": {},
   "source": [
    "## RECONOCIMIENTO INTERVENCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear df_int con las columnas solicitadas\n",
    "requested_columns = ['upid', 'referencia_proceso', 'referencia_contrato', 'bpin', 'identificador', 'fuente_financiacion', \n",
    "                     'tipo_intervencion', 'unidad', 'cantidad', 'estado', 'presupuesto_base', 'avance_obra', 'fecha_inicio',\n",
    "                     'fecha_fin', 'fecha_inauguracion', 'url_proceso', 'descripcion_intervencion', 'observaciones', \n",
    "                     'nombre_centro_gestor', 'clase_up']\n",
    "\n",
    "# Filter only columns that exist in df\n",
    "available_columns = [col for col in requested_columns if col in df.columns]\n",
    "df_int = df[available_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb24952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create intervencion_id with reset counter for each upid\n",
    "df_int['intervencion_id'] = df_int.groupby('upid').cumcount() + 1\n",
    "df_int['intervencion_id'] = df_int['upid'] + '-INT-' + df_int['intervencion_id'].astype(str)\n",
    "\n",
    "# Reorder columns to put intervencion_id after upid\n",
    "cols = ['upid', 'intervencion_id'] + [col for col in df_int.columns if col not in ['upid', 'intervencion_id']]\n",
    "df_int = df_int[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdfaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_int.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each upid appears in df_up\n",
    "upid_counts = df_up['upid'].value_counts()\n",
    "\n",
    "# Filter only the repeated upids (appearing more than once)\n",
    "repeated_upids = upid_counts[upid_counts > 1]\n",
    "\n",
    "print(f\"Total UPIDs in df_up: {len(df_up)}\")\n",
    "print(f\"Unique UPIDs in df_up: {df_up['upid'].nunique()}\")\n",
    "print(f\"Number of repeated UPIDs: {len(repeated_upids)}\")\n",
    "print(f\"\\nRepeated UPIDs and their counts:\")\n",
    "print(repeated_upids.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff38d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9d5ed",
   "metadata": {},
   "source": [
    "## SUBIR UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd94026",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate upids, keeping only the first occurrence\n",
    "df_up_clean = df_up.drop_duplicates(subset=['upid'], keep='first').reset_index(drop=True)\n",
    "\n",
    "print(f\"Records before: {len(df_up)}\")\n",
    "print(f\"Records after removing duplicates: {len(df_up_clean)}\")\n",
    "\n",
    "# Use df_up_clean for Firebase upload instead of df_up\n",
    "records = df_up_clean.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "\n",
    "# Initialize Firebase (only once)\n",
    "if not firebase_admin._apps:\n",
    "    try:\n",
    "        cred = credentials.Certificate(os.path.join(project_root, \"env/calitrack-secret.json\"))\n",
    "        firebase_admin.initialize_app(cred)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Firebase: {e}\")\n",
    "        raise\n",
    "\n",
    "# Get Firestore client\n",
    "db = firestore.client()\n",
    "\n",
    "# Test connection and permissions\n",
    "try:\n",
    "    test_doc = db.collection('unidades_proyecto').document('test').get()\n",
    "    print(\"✓ Firebase connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Firebase permission error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Reference to the collection\n",
    "collection_ref = db.collection('unidades_proyecto')\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "records = df_up_clean.to_dict('records')\n",
    "\n",
    "def _insert_decimal(digits, pos):\n",
    "    return digits[:pos] + \".\" + digits[pos:]\n",
    "\n",
    "def _fix_coord(value, kind):\n",
    "    if value is None:\n",
    "        return None\n",
    "    try:\n",
    "        coord = float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return None\n",
    "    max_abs = 180 if kind == \"lon\" else 90\n",
    "    if abs(coord) <= max_abs:\n",
    "        return coord\n",
    "    sign = -1 if coord < 0 else 1\n",
    "    digits = str(abs(int(round(coord))))\n",
    "    pos = 2 if kind == \"lon\" else 1\n",
    "    if len(digits) <= pos:\n",
    "        return coord\n",
    "    return sign * float(_insert_decimal(digits, pos))\n",
    "\n",
    "def normalize_geometry(geom):\n",
    "    if not isinstance(geom, dict):\n",
    "        return geom\n",
    "    coords = geom.get(\"coordinates\")\n",
    "    if not isinstance(coords, (list, tuple)) or len(coords) != 2:\n",
    "        return geom\n",
    "    lon = _fix_coord(coords[0], \"lon\")\n",
    "    lat = _fix_coord(coords[1], \"lat\")\n",
    "    if lon is None or lat is None:\n",
    "        return geom\n",
    "    return {**geom, \"coordinates\": [lon, lat]}\n",
    "\n",
    "# Upload each record to Firestore\n",
    "batch = db.batch()\n",
    "batch_count = 0\n",
    "uploaded_count = 0\n",
    "\n",
    "for record in records:\n",
    "    doc_id = record.get('upid', f'doc_{uploaded_count}')\n",
    "    doc_ref = collection_ref.document(doc_id)\n",
    "    clean_record = {k: v for k, v in record.items() if v is not None and (not isinstance(v, float) or not pd.isna(v))}\n",
    "    if 'geometry' in clean_record:\n",
    "        clean_record['geometry'] = normalize_geometry(clean_record['geometry'])\n",
    "    \n",
    "    batch.set(doc_ref, clean_record)\n",
    "    batch_count += 1\n",
    "    uploaded_count += 1\n",
    "    \n",
    "    if batch_count >= 500:\n",
    "        try:\n",
    "            batch.commit()\n",
    "            print(f\"Uploaded {uploaded_count} documents...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading batch: {e}\")\n",
    "            raise\n",
    "        batch = db.batch()\n",
    "        batch_count = 0\n",
    "\n",
    "# Commit any remaining documents\n",
    "if batch_count > 0:\n",
    "    try:\n",
    "        batch.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading final batch: {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"✓ Successfully uploaded {uploaded_count} documents to 'unidades_proyecto' collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d13055",
   "metadata": {},
   "source": [
    "## SUBIR INTERVENCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182feaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference to the collection\n",
    "collection_ref = db.collection('intervenciones_unidades_proyecto')\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "records = df_int.to_dict('records')\n",
    "\n",
    "# Upload each record to Firestore\n",
    "batch = db.batch()\n",
    "batch_count = 0\n",
    "uploaded_count = 0\n",
    "\n",
    "for record in records:\n",
    "    doc_id = record.get('intervencion_id', f'doc_{uploaded_count}')\n",
    "    doc_ref = collection_ref.document(doc_id)\n",
    "    clean_record = {k: v for k, v in record.items() if v is not None and (not isinstance(v, float) or not pd.isna(v))}\n",
    "    \n",
    "    batch.set(doc_ref, clean_record)\n",
    "    batch_count += 1\n",
    "    uploaded_count += 1\n",
    "    if batch_count >= 500:\n",
    "        try:\n",
    "            batch.commit()\n",
    "            print(f\"Uploaded {uploaded_count} documents...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading batch: {e}\")\n",
    "            raise\n",
    "        batch = db.batch()\n",
    "        batch_count = 0\n",
    "\n",
    "# Commit any remaining documents\n",
    "if batch_count > 0:\n",
    "    try:\n",
    "        batch.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading final batch: {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"✓ Successfully uploaded {uploaded_count} documents to 'intervenciones_unidades_proyecto' collection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
