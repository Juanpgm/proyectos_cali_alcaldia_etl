# -*- coding: utf-8 -*-
"""
Unidades de Proyecto ETL Pipeline

Pipeline completo de ExtracciÃ³n, TransformaciÃ³n y Carga (ETL) para unidades de proyecto.
Implementa programaciÃ³n funcional para un cÃ³digo limpio, eficiente y escalable.
Incluye verificaciÃ³n incremental para cargar solo datos nuevos o modificados.

Funcionalidades:
- ExtracciÃ³n de datos desde Google Sheets con autenticaciÃ³n segura
- TransformaciÃ³n de datos con procesamiento geoespacial
- VerificaciÃ³n de cambios contra Firebase (carga incremental)
- Carga batch optimizada a Firebase Firestore
- Logging detallado y manejo de errores
- Compatible con GitHub Actions para automatizaciÃ³n
"""

import os
import sys
import json
import pandas as pd
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Callable
from functools import reduce, partial, wraps
import hashlib

# Agregar rutas necesarias al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Importar mÃ³dulos del proyecto
from extraction_app.data_extraction_unidades_proyecto import extract_and_save_unidades_proyecto
from transformation_app.data_transformation_unidades_proyecto import transform_and_save_unidades_proyecto
from load_app.data_loading_unidades_proyecto import load_unidades_proyecto_to_firebase
from database.config import get_firestore_client, secure_log


# Utilidades de programaciÃ³n funcional
def compose(*functions: Callable) -> Callable:
    """Compone mÃºltiples funciones en una sola funciÃ³n."""
    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)


def pipe(value: Any, *functions: Callable) -> Any:
    """Aplica una secuencia de funciones a un valor (operador pipe)."""
    return reduce(lambda acc, func: func(acc), functions, value)


def curry(func: Callable) -> Callable:
    """Convierte una funciÃ³n a una versiÃ³n currificada para aplicaciÃ³n parcial."""
    @wraps(func)
    def curried(*args, **kwargs):
        if len(args) + len(kwargs) >= func.__code__.co_argcount:
            return func(*args, **kwargs)
        return lambda *more_args, **more_kwargs: curried(*(args + more_args), **dict(kwargs, **more_kwargs))
    return curried


def safe_execute(func: Callable, default_value: Any = None) -> Callable:
    """Ejecuta funciones de forma segura con manejo de errores."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            result = func(*args, **kwargs)
            return result
        except Exception as e:
            print(f"âŒ Error en {func.__name__}: {e}")
            import traceback
            traceback.print_exc()
            return default_value
    return wrapper


def log_step(step_name: str) -> Callable:
    """Decorador para logging de pasos del pipeline."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            print(f"\n{'='*60}")
            print(f"ðŸ“Š PASO: {step_name}")
            print(f"{'='*60}")
            start_time = datetime.now()
            
            result = func(*args, **kwargs)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # Mejor validaciÃ³n que maneja DataFrames
            is_success = False
            if result is not None:
                if hasattr(result, 'empty'):  # Es un DataFrame
                    is_success = not result.empty
                elif isinstance(result, bool):
                    is_success = result
                elif isinstance(result, (list, dict, str)):
                    is_success = len(result) > 0
                else:
                    is_success = True  # Para otros tipos no nulos
            
            if is_success:
                print(f"âœ… {step_name} completado en {duration:.2f}s")
            else:
                print(f"âŒ {step_name} fallÃ³ despuÃ©s de {duration:.2f}s")
            
            return result
        return wrapper
    return decorator


# Funciones para verificaciÃ³n incremental de datos
def calculate_record_hash(record: Dict[str, Any]) -> str:
    """
    Calcula un hash Ãºnico para un registro para detectar cambios.
    Maneja tanto features GeoJSON como documentos de Firebase.
    
    Args:
        record: Diccionario con los datos del registro (GeoJSON feature o documento Firebase)
        
    Returns:
        Hash MD5 del registro como string
    """
    # Determinar si es un GeoJSON feature o documento de Firebase
    if record.get('type') == 'Feature':
        # Es un GeoJSON feature - usar solo las propiedades para comparar
        properties = record.get('properties', {})
        geometry = record.get('geometry')
        
        # Crear objeto consistente para hash
        hash_data = {
            'properties': properties,
            'geometry': geometry
        }
    else:
        # Es un documento de Firebase - extraer propiedades y geometrÃ­a
        properties = record.get('properties', {})
        geometry = record.get('geometry')
        
        hash_data = {
            'properties': properties,
            'geometry': geometry
        }
    
    # Excluir campos de metadata que cambian automÃ¡ticamente
    excluded_fields = ['created_at', 'updated_at', 'processed_timestamp', 
                      'has_geometry', 'geometry_type']
    
    # Limpiar las propiedades de campos de metadata
    if 'properties' in hash_data and hash_data['properties']:
        cleaned_properties = {
            k: v for k, v in hash_data['properties'].items() 
            if k not in excluded_fields and v is not None
        }
        hash_data['properties'] = cleaned_properties
    
    # Convertir a JSON string ordenado para hash consistente
    record_str = json.dumps(hash_data, sort_keys=True, default=str)
    
    # Calcular hash MD5
    return hashlib.md5(record_str.encode('utf-8')).hexdigest()


@safe_execute
def get_existing_firebase_data(collection_name: str = "unidades_proyecto") -> Dict[str, Dict[str, Any]]:
    """
    Obtiene los datos existentes en Firebase para comparaciÃ³n.
    
    Args:
        collection_name: Nombre de la colecciÃ³n en Firebase
        
    Returns:
        Diccionario con {doc_id: {data_hash, last_updated}} o {} si falla
    """
    print(f"ðŸ” Obteniendo datos existentes de Firebase colecciÃ³n '{collection_name}'...")
    
    try:
        db = get_firestore_client()
        if not db:
            print("âŒ No se pudo conectar a Firebase")
            return {}
        
        collection_ref = db.collection(collection_name)
        
        # Obtener solo los campos necesarios para comparaciÃ³n (mÃ¡s eficiente)
        existing_data = {}
        
        docs = collection_ref.stream()
        doc_count = 0
        
        for doc in docs:
            doc_data = doc.to_dict()
            
            # Calcular hash de los datos existentes
            data_hash = calculate_record_hash(doc_data)
            
            existing_data[doc.id] = {
                'hash': data_hash,
                'updated_at': doc_data.get('updated_at'),
                'properties': doc_data.get('properties', {})
            }
            
            doc_count += 1
        
        print(f"âœ… Obtenidos {doc_count} registros existentes de Firebase")
        return existing_data
        
    except Exception as e:
        print(f"âŒ Error obteniendo datos de Firebase: {e}")
        return {}


def compare_and_filter_changes(
    new_features: List[Dict[str, Any]], 
    existing_data: Dict[str, Dict[str, Any]]
) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
    """
    Compara datos nuevos con existentes y filtra solo los cambios.
    
    Args:
        new_features: Lista de features nuevos desde la transformaciÃ³n
        existing_data: Datos existentes en Firebase
        
    Returns:
        Tupla de (features_to_upload, change_summary)
    """
    print(f"ðŸ”„ Comparando {len(new_features)} registros nuevos con {len(existing_data)} existentes...")
    
    features_to_upload = []
    change_summary = {
        'new_records': 0,
        'modified_records': 0,
        'unchanged_records': 0,
        'total_processed': len(new_features)
    }
    
    for feature in new_features:
        try:
            # Obtener ID del documento
            properties = feature.get('properties', {})
            doc_id = None
            
            # Buscar ID usando la misma lÃ³gica que en load_app
            if properties.get('upid'):
                doc_id = str(properties['upid']).strip()
            elif properties.get('identificador'):
                doc_id = f"ID-{str(properties['identificador']).strip()}"
            elif properties.get('bpin'):
                doc_id = f"BPIN-{str(properties['bpin']).strip()}"
            
            if not doc_id:
                # Si no hay ID, es un registro nuevo
                features_to_upload.append(feature)
                change_summary['new_records'] += 1
                continue
            
            # Calcular hash del nuevo registro
            new_hash = calculate_record_hash(feature)
            
            # Verificar si existe en Firebase
            if doc_id in existing_data:
                existing_hash = existing_data[doc_id]['hash']
                
                if new_hash != existing_hash:
                    # Registro modificado
                    features_to_upload.append(feature)
                    change_summary['modified_records'] += 1
                else:
                    # Registro sin cambios
                    change_summary['unchanged_records'] += 1
            else:
                # Registro nuevo
                features_to_upload.append(feature)
                change_summary['new_records'] += 1
                
        except Exception as e:
            print(f"âš ï¸ Error comparando registro: {e}")
            # En caso de error, incluir el registro para estar seguros
            features_to_upload.append(feature)
            change_summary['new_records'] += 1
    
    print(f"ðŸ“Š Resumen de cambios:")
    print(f"  âž• Nuevos: {change_summary['new_records']}")
    print(f"  ðŸ”„ Modificados: {change_summary['modified_records']}")
    print(f"  âœ… Sin cambios: {change_summary['unchanged_records']}")
    print(f"  ðŸ“¤ Total a cargar: {len(features_to_upload)}")
    
    return features_to_upload, change_summary


def create_incremental_geojson(
    features_to_upload: List[Dict[str, Any]], 
    output_path: str
) -> bool:
    """
    Crea un archivo GeoJSON con solo los registros que necesitan actualizarse.
    
    Args:
        features_to_upload: Lista de features a cargar
        output_path: Ruta donde guardar el archivo GeoJSON
        
    Returns:
        True si se guardÃ³ exitosamente, False en caso contrario
    """
    try:
        # Crear FeatureCollection incremental
        incremental_geojson = {
            "type": "FeatureCollection",
            "features": features_to_upload,
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "feature_count": len(features_to_upload),
                "type": "incremental_update"
            }
        }
        
        # Crear directorio si no existe
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Guardar archivo
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(incremental_geojson, f, indent=2, ensure_ascii=False, default=str)
        
        file_size_kb = os.path.getsize(output_path) / 1024
        print(f"ðŸ’¾ GeoJSON incremental guardado: {os.path.basename(output_path)} ({file_size_kb:.1f} KB)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error guardando GeoJSON incremental: {e}")
        return False


# Funciones principales del pipeline
@log_step("EXTRACCIÃ“N DE DATOS")
@safe_execute
def run_extraction() -> Optional[pd.DataFrame]:
    """
    Ejecuta el proceso de extracciÃ³n de datos desde Google Sheets.
    
    Returns:
        DataFrame con los datos extraÃ­dos o None si falla
    """
    return extract_and_save_unidades_proyecto()


@log_step("TRANSFORMACIÃ“N DE DATOS")
@safe_execute
def run_transformation() -> Optional[pd.DataFrame]:
    """
    Ejecuta el proceso de transformaciÃ³n de datos.
    
    Returns:
        DataFrame con los datos transformados o None si falla
    """
    return transform_and_save_unidades_proyecto()


@log_step("VERIFICACIÃ“N INCREMENTAL")
@safe_execute
def verify_and_prepare_incremental_load(
    geojson_path: str,
    collection_name: str = "unidades_proyecto"
) -> Tuple[Optional[str], Optional[Dict[str, int]]]:
    """
    Verifica cambios y prepara carga incremental.
    
    Args:
        geojson_path: Ruta al archivo GeoJSON con todos los datos
        collection_name: Nombre de la colecciÃ³n en Firebase
        
    Returns:
        Tupla de (ruta_geojson_incremental, resumen_cambios) o (None, None) si falla
    """
    try:
        # Cargar GeoJSON completo
        if not os.path.exists(geojson_path):
            print(f"âŒ Archivo GeoJSON no encontrado: {geojson_path}")
            return None, None
        
        with open(geojson_path, 'r', encoding='utf-8') as f:
            geojson_data = json.load(f)
        
        new_features = geojson_data.get('features', [])
        if not new_features:
            print("âš ï¸ No hay features en el GeoJSON")
            return None, None
        
        # Obtener datos existentes de Firebase
        existing_data = get_existing_firebase_data(collection_name)
        
        # Comparar y filtrar cambios
        features_to_upload, change_summary = compare_and_filter_changes(new_features, existing_data)
        
        if not features_to_upload:
            print("âœ… No hay cambios para cargar. Todos los datos estÃ¡n actualizados.")
            return None, change_summary
        
        # Crear GeoJSON incremental
        incremental_path = geojson_path.replace('.geojson', '_incremental.geojson')
        success = create_incremental_geojson(features_to_upload, incremental_path)
        
        if success:
            return incremental_path, change_summary
        else:
            return None, None
            
    except Exception as e:
        print(f"âŒ Error en verificaciÃ³n incremental: {e}")
        return None, None


@log_step("CARGA INCREMENTAL A FIREBASE")
@safe_execute
def run_incremental_load(incremental_geojson_path: str, collection_name: str = "unidades_proyecto") -> bool:
    """
    Ejecuta la carga incremental a Firebase.
    
    Args:
        incremental_geojson_path: Ruta al GeoJSON con solo los cambios
        collection_name: Nombre de la colecciÃ³n en Firebase
        
    Returns:
        True si la carga fue exitosa, False en caso contrario
    """
    if not incremental_geojson_path or not os.path.exists(incremental_geojson_path):
        print("âŒ Archivo GeoJSON incremental no disponible")
        return False
    
    return load_unidades_proyecto_to_firebase(
        input_file=incremental_geojson_path,
        collection_name=collection_name,
        batch_size=100
    )


# Pipeline principal
def create_unidades_proyecto_pipeline() -> Callable[[], Dict[str, Any]]:
    """
    Crea el pipeline completo para unidades de proyecto.
    
    Returns:
        FunciÃ³n del pipeline configurada
    """
    
    def pipeline(collection_name: str = "unidades_proyecto") -> Dict[str, Any]:
        """
        Pipeline ETL completo para unidades de proyecto con carga incremental.
        
        Args:
            collection_name: Nombre de la colecciÃ³n en Firebase
            
        Returns:
            Diccionario con resultados del pipeline
        """
        pipeline_start = datetime.now()
        
        results = {
            'success': False,
            'start_time': pipeline_start.isoformat(),
            'end_time': None,
            'duration_seconds': None,
            'extraction_success': False,
            'transformation_success': False,
            'incremental_check_success': False,
            'load_success': False,
            'records_processed': 0,
            'records_uploaded': 0,
            'change_summary': None,
            'errors': []
        }
        
        try:
            print("ðŸš€ INICIANDO PIPELINE ETL UNIDADES DE PROYECTO")
            print("="*80)
            print(f"â° Inicio: {pipeline_start.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ðŸ—‚ï¸ ColecciÃ³n destino: {collection_name}")
            
            # PASO 1: ExtracciÃ³n
            extraction_result = run_extraction()
            if extraction_result is None or (hasattr(extraction_result, 'empty') and extraction_result.empty):
                results['errors'].append("Fallo en extracciÃ³n de datos")
                return results
            
            results['extraction_success'] = True
            results['records_processed'] = len(extraction_result) if extraction_result is not None else 0
            
            # PASO 2: TransformaciÃ³n
            transformation_result = run_transformation()
            if transformation_result is None or (hasattr(transformation_result, 'empty') and transformation_result.empty):
                results['errors'].append("Fallo en transformaciÃ³n de datos")
                return results
            
            results['transformation_success'] = True
            
            # PASO 3: VerificaciÃ³n incremental
            geojson_path = os.path.join(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                "transformation_app",
                "app_outputs",
                "unidades_proyecto_outputs",
                "unidades_proyecto.geojson"
            )
            
            incremental_path, change_summary = verify_and_prepare_incremental_load(
                geojson_path, collection_name
            )
            
            results['incremental_check_success'] = True
            results['change_summary'] = change_summary
            
            # PASO 4: Carga incremental (solo si hay cambios)
            if incremental_path and change_summary:
                load_success = run_incremental_load(incremental_path, collection_name)
                results['load_success'] = load_success
                
                if load_success and change_summary:
                    results['records_uploaded'] = (
                        change_summary.get('new_records', 0) + 
                        change_summary.get('modified_records', 0)
                    )
                
                # Limpiar archivo incremental temporal
                try:
                    if os.path.exists(incremental_path):
                        os.remove(incremental_path)
                        print(f"ðŸ—‘ï¸ Archivo temporal eliminado: {os.path.basename(incremental_path)}")
                except:
                    pass
                    
            else:
                print("âœ… No hay cambios para cargar - datos actualizados")
                results['load_success'] = True  # No habÃ­a nada que cargar
                results['records_uploaded'] = 0
            
            # Calcular resultados finales
            pipeline_end = datetime.now()
            results['end_time'] = pipeline_end.isoformat()
            results['duration_seconds'] = (pipeline_end - pipeline_start).total_seconds()
            results['success'] = (
                results['extraction_success'] and 
                results['transformation_success'] and 
                results['incremental_check_success'] and 
                results['load_success']
            )
            
            return results
            
        except Exception as e:
            results['errors'].append(f"Error general del pipeline: {str(e)}")
            pipeline_end = datetime.now()
            results['end_time'] = pipeline_end.isoformat()
            results['duration_seconds'] = (pipeline_end - pipeline_start).total_seconds()
            return results
    
    return pipeline


def print_pipeline_summary(results: Dict[str, Any]):
    """
    Imprime un resumen detallado de los resultados del pipeline.
    
    Args:
        results: Diccionario con los resultados del pipeline
    """
    print(f"\n{'='*80}")
    print("ðŸ“Š RESUMEN DEL PIPELINE ETL")
    print("="*80)
    
    # Estado general
    status_icon = "âœ…" if results['success'] else "âŒ"
    print(f"{status_icon} Estado general: {'EXITOSO' if results['success'] else 'FALLIDO'}")
    
    # Tiempos
    if results['start_time']:
        start_time = datetime.fromisoformat(results['start_time'])
        print(f"â° Inicio: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    if results['end_time']:
        end_time = datetime.fromisoformat(results['end_time'])
        print(f"ðŸ Fin: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    if results['duration_seconds']:
        duration = results['duration_seconds']
        minutes = int(duration // 60)
        seconds = int(duration % 60)
        print(f"â±ï¸ DuraciÃ³n: {minutes}m {seconds}s")
    
    # Pasos del pipeline
    print(f"\nðŸ”„ Pasos ejecutados:")
    steps = [
        ("ExtracciÃ³n", results['extraction_success']),
        ("TransformaciÃ³n", results['transformation_success']),
        ("VerificaciÃ³n incremental", results['incremental_check_success']),
        ("Carga a Firebase", results['load_success'])
    ]
    
    for step_name, success in steps:
        icon = "âœ…" if success else "âŒ"
        print(f"  {icon} {step_name}")
    
    # EstadÃ­sticas de datos
    print(f"\nðŸ“ˆ EstadÃ­sticas:")
    print(f"  ðŸ“¥ Registros procesados: {results.get('records_processed', 0)}")
    print(f"  ðŸ“¤ Registros cargados: {results.get('records_uploaded', 0)}")
    
    # Resumen de cambios
    if results.get('change_summary'):
        summary = results['change_summary']
        print(f"\nðŸ”„ Resumen de cambios:")
        print(f"  âž• Nuevos: {summary.get('new_records', 0)}")
        print(f"  ðŸ”„ Modificados: {summary.get('modified_records', 0)}")
        print(f"  âœ… Sin cambios: {summary.get('unchanged_records', 0)}")
    
    # Errores
    if results.get('errors'):
        print(f"\nâŒ Errores encontrados:")
        for i, error in enumerate(results['errors'], 1):
            print(f"  {i}. {error}")
    
    # Mensaje final
    if results['success']:
        print(f"\nðŸŽ‰ Pipeline completado exitosamente!")
        if results.get('records_uploaded', 0) > 0:
            print(f"âœ¨ {results['records_uploaded']} registros actualizados en Firebase")
        else:
            print("âœ¨ Todos los datos estaban actualizados")
    else:
        print(f"\nðŸ’¥ Pipeline fallÃ³. Revisa los errores arriba.")
    
    print("="*80)


def run_unidades_proyecto_pipeline(collection_name: str = "unidades_proyecto") -> bool:
    """
    FunciÃ³n principal para ejecutar el pipeline completo de unidades de proyecto.
    
    Args:
        collection_name: Nombre de la colecciÃ³n en Firebase
        
    Returns:
        True si el pipeline fue exitoso, False en caso contrario
    """
    # Crear y ejecutar pipeline
    pipeline = create_unidades_proyecto_pipeline()
    results = pipeline(collection_name)
    
    # Mostrar resumen
    print_pipeline_summary(results)
    
    return results['success']


def main():
    """FunciÃ³n principal para pruebas del pipeline."""
    return run_unidades_proyecto_pipeline()


if __name__ == "__main__":
    """
    Bloque de ejecuciÃ³n principal para probar el pipeline completo.
    """
    print("ðŸš€ Iniciando pipeline ETL de Unidades de Proyecto...")
    
    # Ejecutar pipeline completo
    success = main()
    
    if success:
        print("\nðŸŽ¯ PIPELINE COMPLETADO EXITOSAMENTE")
        print("âœ¨ Datos de unidades de proyecto actualizados")
    else:
        print("\nðŸ’¥ PIPELINE FALLÃ“")
        print("ðŸ”§ Revisa los errores y logs anteriores")
    
    # CÃ³digo de salida para scripts automatizados
    sys.exit(0 if success else 1)
