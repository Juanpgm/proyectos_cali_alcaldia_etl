#!/usr/bin/env python3
"""
Cargador Final de Datos - Alcald√≠a de Santiago de Cali ETL
=========================================================

Cargador final optimizado que ejecuta la carga completa usando
los archivos SQL generados por el analizador final.

Caracter√≠sticas:
- Ejecuci√≥n robusta con manejo avanzado de errores
- Carga optimizada por lotes
- Validaci√≥n de integridad de datos
- Reporte detallado de resultados
- Recuperaci√≥n autom√°tica de errores menores

Versi√≥n: 3.0.0 - Final
"""

import json
import psycopg2
import psycopg2.extras
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import logging
import sys
import traceback
import time

# Agregar el directorio ra√≠z al path
sys.path.append(str(Path(__file__).parent.parent.parent))

from database_management.core.config import get_database_config

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('final_data_load.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class DataLoadResult:
    """Clase para almacenar resultados de carga de datos"""
    def __init__(self):
        self.start_time = datetime.now()
        self.end_time = None
        self.duration = None
        self.total_tables = 0
        self.successful_tables = []
        self.failed_tables = []
        self.total_records = 0
        self.errors = []
        self.warnings = []
    
    def finish(self):
        self.end_time = datetime.now()
        self.duration = self.end_time - self.start_time
    
    def add_success(self, table_name: str, records: int, file_path: str):
        self.successful_tables.append({
            'table': table_name,
            'records': records,
            'file': file_path
        })
        self.total_records += records
    
    def add_failure(self, table_name: str, error: str):
        self.failed_tables.append({
            'table': table_name,
            'error': error
        })
    
    def add_error(self, error: str):
        self.errors.append(error)
    
    def add_warning(self, warning: str):
        self.warnings.append(warning)
    
    @property
    def success_rate(self) -> float:
        if self.total_tables == 0:
            return 0.0
        return (len(self.successful_tables) / self.total_tables) * 100

def execute_sql_file_robust(cursor, sql_file_path: Path, description: str) -> bool:
    """Ejecuta un archivo SQL con manejo robusto de errores"""
    try:
        logger.info(f"üîß {description}")
        
        if not sql_file_path.exists():
            logger.warning(f"‚ö†Ô∏è Archivo no encontrado: {sql_file_path}")
            return False
        
        sql_content = sql_file_path.read_text(encoding='utf-8').strip()
        
        if not sql_content:
            logger.warning(f"‚ö†Ô∏è Archivo vac√≠o: {sql_file_path}")
            return False
        
        # Ejecutar SQL
        cursor.execute(sql_content)
        logger.info(f"‚úÖ {description} - Completado")
        return True
        
    except psycopg2.Error as e:
        logger.error(f"‚ùå Error SQL en {description}: {e}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Error general en {description}: {e}")
        return False

def load_json_data_optimized(cursor, table_name: str, json_file_path: Path) -> int:
    """
    Carga datos JSON con optimizaciones y manejo robusto de errores
    """
    try:
        logger.info(f"üìÅ Cargando: {table_name} ‚Üê {json_file_path.name}")
        
        # Verificar archivo
        if not json_file_path.exists():
            logger.warning(f"‚ö†Ô∏è Archivo no encontrado: {json_file_path}")
            return 0
        
        # Cargar datos JSON
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list) or len(data) == 0:
            logger.warning(f"‚ö†Ô∏è Sin datos en {json_file_path}")
            return 0
        
        # Obtener metadatos de la tabla
        cursor.execute("""
            SELECT column_name, data_type, is_nullable, column_default
            FROM information_schema.columns 
            WHERE table_name = %s 
            AND table_schema = 'public'
            AND column_name NOT IN ('id', 'created_at', 'updated_at', 'version', 'is_active')
            ORDER BY ordinal_position;
        """, (table_name,))
        
        columns_info = cursor.fetchall()
        
        if not columns_info:
            logger.warning(f"‚ö†Ô∏è No se encontraron columnas para {table_name}")
            return 0
        
        column_names = [col[0] for col in columns_info]
        
        # Preparar datos para inserci√≥n
        valid_rows = []
        error_count = 0
        
        for i, record in enumerate(data):
            if not isinstance(record, dict):
                error_count += 1
                continue
            
            row_values = []
            for col_name in column_names:
                value = record.get(col_name)
                
                # Normalizar valores
                if value is None or value == "" or str(value).strip().lower() in ['null', 'none', 'nan']:
                    row_values.append(None)
                else:
                    row_values.append(value)
            
            valid_rows.append(tuple(row_values))
        
        if not valid_rows:
            logger.warning(f"‚ö†Ô∏è No hay filas v√°lidas en {table_name}")
            return 0
        
        if error_count > 0:
            logger.warning(f"‚ö†Ô∏è {error_count} registros inv√°lidos omitidos en {table_name}")
        
        # Preparar SQL de inserci√≥n
        placeholders = ", ".join(["%s"] * len(column_names))
        columns_str = ", ".join(column_names)
        
        insert_sql = f"""
            INSERT INTO {table_name} ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT DO NOTHING;
        """
        
        # Inserci√≥n por lotes con progreso
        batch_size = 1000
        total_inserted = 0
        batch_count = 0
        
        try:
            for i in range(0, len(valid_rows), batch_size):
                batch = valid_rows[i:i + batch_size]
                
                # Ejecutar lote
                cursor.executemany(insert_sql, batch)
                
                # Contar insertados (aproximado)
                inserted_in_batch = len(batch)
                total_inserted += inserted_in_batch
                batch_count += 1
                
                # Log de progreso cada 5 lotes
                if batch_count % 5 == 0:
                    logger.info(f"  üìä {total_inserted:,} filas procesadas en {table_name}")
        
        except psycopg2.Error as e:
            logger.error(f"‚ùå Error de inserci√≥n en {table_name}: {e}")
            return 0
        
        logger.info(f"‚úÖ {table_name}: {total_inserted:,} registros cargados")
        return total_inserted
        
    except Exception as e:
        logger.error(f"‚ùå Error cargando {table_name}: {e}")
        return 0

def get_json_table_mapping(data_dir: Path) -> Dict[str, Path]:
    """Mapea nombres de tabla a archivos JSON"""
    mapping = {}
    
    for json_file in data_dir.rglob("*.json"):
        try:
            # Verificar que tiene datos v√°lidos
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list) and len(data) > 0:
                # Generar nombre de tabla
                table_name = json_file.stem.lower()
                table_name = table_name.replace('-', '_').replace(' ', '_')
                
                # Limpiar caracteres especiales
                import re
                table_name = re.sub(r'[^a-zA-Z0-9_]', '_', table_name)
                
                mapping[table_name] = json_file
                
        except Exception:
            continue  # Omitir archivos problem√°ticos
    
    return mapping

def execute_final_data_load() -> DataLoadResult:
    """Ejecuta la carga final completa de datos"""
    result = DataLoadResult()
    
    logger.info("üöÄ INICIANDO CARGA FINAL DE DATOS DEL WAREHOUSE")
    logger.info("=" * 60)
    
    try:
        # Configuraci√≥n de base de datos
        config = get_database_config()
        logger.info(f"üîó Conectando a: {config.host}:{config.port}/{config.database}")
        
        # Conexi√≥n
        conn = psycopg2.connect(
            host=config.host,
            port=config.port,
            database=config.database,
            user=config.user,
            password=config.password
        )
        conn.autocommit = True
        cursor = conn.cursor()
        
        # Rutas
        project_root = Path(__file__).parent.parent.parent
        sql_dir = project_root / "database_management" / "generated_sql"
        data_dir = project_root / "transformation_app" / "app_outputs"
        
        # FASE 1: Setup del warehouse
        logger.info("üìã FASE 1: Configuraci√≥n del warehouse")
        setup_file = sql_dir / "01_warehouse_setup.sql"
        if not execute_sql_file_robust(cursor, setup_file, "Setup del warehouse"):
            result.add_error("Error en setup del warehouse")
        
        # FASE 2: Crear tablas
        logger.info("üìã FASE 2: Creaci√≥n de tablas")
        tables_file = sql_dir / "02_create_tables.sql"
        if not execute_sql_file_robust(cursor, tables_file, "Creaci√≥n de tablas"):
            result.add_error("Error cr√≠tico: No se pudieron crear las tablas")
            raise Exception("Error cr√≠tico en creaci√≥n de tablas")
        
        # FASE 3: Crear √≠ndices
        logger.info("üìã FASE 3: Creaci√≥n de √≠ndices")
        indexes_file = sql_dir / "03_create_indexes.sql"
        if not execute_sql_file_robust(cursor, indexes_file, "Creaci√≥n de √≠ndices"):
            result.add_warning("Algunos √≠ndices no se crearon correctamente")
        
        # FASE 4: Crear triggers
        logger.info("üìã FASE 4: Creaci√≥n de triggers")
        triggers_file = sql_dir / "04_create_triggers.sql"
        if not execute_sql_file_robust(cursor, triggers_file, "Creaci√≥n de triggers"):
            result.add_warning("Algunos triggers no se crearon correctamente")
        
        # FASE 5: Mapear archivos JSON
        logger.info("üìã FASE 5: Mapeo de datos")
        table_json_mapping = get_json_table_mapping(data_dir)
        result.total_tables = len(table_json_mapping)
        
        logger.info(f"üìä Encontradas {len(table_json_mapping)} tablas para cargar")
        
        # FASE 6: Cargar datos
        logger.info("üìã FASE 6: Carga de datos")
        
        for table_name, json_file in table_json_mapping.items():
            try:
                # Verificar que la tabla existe
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = %s
                    );
                """, (table_name,))
                
                table_exists = cursor.fetchone()[0]
                
                if not table_exists:
                    logger.warning(f"‚ö†Ô∏è Tabla {table_name} no existe, omitiendo")
                    result.add_failure(table_name, "tabla no existe")
                    continue
                
                # Cargar datos
                start_time = time.time()
                records_loaded = load_json_data_optimized(cursor, table_name, json_file)
                load_time = time.time() - start_time
                
                if records_loaded > 0:
                    result.add_success(table_name, records_loaded, str(json_file))
                    logger.info(f"  ‚è±Ô∏è Tiempo: {load_time:.2f}s ({records_loaded/load_time:.0f} rec/s)")
                else:
                    result.add_failure(table_name, "no se cargaron registros")
                
            except Exception as e:
                error_msg = str(e)
                logger.error(f"‚ùå Error en tabla {table_name}: {error_msg}")
                result.add_failure(table_name, error_msg)
        
        # FASE 7: Crear vistas anal√≠ticas
        logger.info("üìã FASE 7: Vistas anal√≠ticas")
        analytics_file = sql_dir / "05_analytics_views.sql"
        if not execute_sql_file_robust(cursor, analytics_file, "Vistas anal√≠ticas"):
            result.add_warning("Error en creaci√≥n de vistas anal√≠ticas")
        
        # FASE 8: Validaci√≥n final
        logger.info("üìã FASE 8: Validaci√≥n final")
        cursor.execute("""
            SELECT 
                schemaname,
                tablename,
                n_live_tup as row_count
            FROM pg_stat_user_tables 
            WHERE schemaname = 'public'
            AND tablename NOT LIKE 'pg_%'
            ORDER BY n_live_tup DESC;
        """)
        
        table_stats = cursor.fetchall()
        logger.info(f"üìä Estad√≠sticas finales de {len(table_stats)} tablas:")
        for schema, table, rows in table_stats[:10]:  # Top 10
            logger.info(f"  ‚Ä¢ {table}: {rows:,} filas")
        
        # Cerrar conexi√≥n
        cursor.close()
        conn.close()
        
        result.finish()
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico: {e}")
        result.add_error(f"Error cr√≠tico: {e}")
        result.finish()
        return result

def print_final_report(result: DataLoadResult):
    """Imprime reporte final detallado"""
    print("\n" + "=" * 80)
    print("üéâ REPORTE FINAL - CARGA DE DATOS WAREHOUSE ETL")
    print("=" * 80)
    
    # Tiempos
    print(f"‚è±Ô∏è Inicio: {result.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚è±Ô∏è Fin: {result.end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚è±Ô∏è Duraci√≥n total: {result.duration}")
    
    # Estad√≠sticas principales
    print(f"\nüìä RESUMEN EJECUTIVO:")
    print(f"  ‚Ä¢ Total de tablas: {result.total_tables}")
    print(f"  ‚Ä¢ Tablas exitosas: {len(result.successful_tables)}")
    print(f"  ‚Ä¢ Tablas fallidas: {len(result.failed_tables)}")
    print(f"  ‚Ä¢ Total registros: {result.total_records:,}")
    print(f"  ‚Ä¢ Tasa de √©xito: {result.success_rate:.1f}%")
    
    # Tablas exitosas
    if result.successful_tables:
        print(f"\n‚úÖ TABLAS CARGADAS EXITOSAMENTE ({len(result.successful_tables)}):")
        sorted_tables = sorted(result.successful_tables, key=lambda x: x['records'], reverse=True)
        for table_info in sorted_tables:
            print(f"  ‚Ä¢ {table_info['table']}: {table_info['records']:,} registros")
    
    # Tablas fallidas
    if result.failed_tables:
        print(f"\n‚ùå TABLAS CON ERRORES ({len(result.failed_tables)}):")
        for table_info in result.failed_tables:
            print(f"  ‚Ä¢ {table_info['table']}: {table_info['error']}")
    
    # Advertencias
    if result.warnings:
        print(f"\n‚ö†Ô∏è ADVERTENCIAS ({len(result.warnings)}):")
        for warning in result.warnings:
            print(f"  ‚Ä¢ {warning}")
    
    # Errores
    if result.errors:
        print(f"\nüö® ERRORES CR√çTICOS ({len(result.errors)}):")
        for error in result.errors:
            print(f"  ‚Ä¢ {error}")
    
    # Estado final
    if result.success_rate >= 90:
        status = "üü¢ EXCELENTE"
    elif result.success_rate >= 70:
        status = "üü° ACEPTABLE"
    else:
        status = "üî¥ REQUIERE ATENCI√ìN"
    
    print(f"\nüìà ESTADO FINAL: {status}")
    print(f"üìÅ Logs detallados en: final_data_load.log")
    print("=" * 80)

def main():
    """Funci√≥n principal"""
    try:
        result = execute_final_data_load()
        print_final_report(result)
        
        # Retornar c√≥digo de salida basado en el √©xito
        if result.success_rate >= 70:  # 70% o m√°s es aceptable
            return 0
        else:
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Error en funci√≥n principal: {e}")
        print(f"‚ùå Error cr√≠tico: {e}")
        return 1

if __name__ == "__main__":
    exit(main())