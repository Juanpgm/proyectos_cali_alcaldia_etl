# Gu√≠a de Despliegue Serverless - Pipeline ETL Unidades de Proyecto

## üìã Tabla de Contenidos

1. [Visi√≥n General](#visi√≥n-general)
2. [Arquitectura Serverless](#arquitectura-serverless)
3. [Prerrequisitos](#prerrequisitos)
4. [Configuraci√≥n de Credenciales](#configuraci√≥n-de-credenciales)
5. [Despliegue de Cloud Function](#despliegue-de-cloud-function)
6. [Configuraci√≥n de Cloud Scheduler](#configuraci√≥n-de-cloud-scheduler)
7. [Variables de Entorno](#variables-de-entorno)
8. [Testing y Validaci√≥n](#testing-y-validaci√≥n)
9. [Monitoreo y Logs](#monitoreo-y-logs)
10. [Troubleshooting](#troubleshooting)

---

## üéØ Visi√≥n General

Este sistema implementa un pipeline ETL 100% serverless que:

- ‚úÖ Se ejecuta **cada hora** autom√°ticamente desde medianoche (cron: `0 * * * *`)
- ‚úÖ Extrae datos desde **Google Drive**
- ‚úÖ Transforma datos con procesamiento geoespacial
- ‚úÖ Carga a **Firebase Firestore** con actualizaciones selectivas
- ‚úÖ Usa **upid** como identificador √∫nico
- ‚úÖ Solo actualiza campos que han cambiado (no todo siempre)
- ‚úÖ No expone credenciales (usa Secret Manager)

---

## üèóÔ∏è Arquitectura Serverless

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Cloud Scheduler                              ‚îÇ
‚îÇ                   Cron: 0 * * * * (cada hora)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ HTTP Trigger
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Cloud Function: etl_pipeline_hourly                ‚îÇ
‚îÇ                    (Python 3.11, Gen 2)                         ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ETAPA 1: Extracci√≥n (Google Drive)                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - M√≥dulo: transformation_app (incluye extracci√≥n)       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚îÇ                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ETAPA 2: Transformaci√≥n                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Procesamiento geoespacial                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Validaci√≥n de coordenadas                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Generaci√≥n de m√©tricas                                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚îÇ                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ETAPA 3: Carga a Firebase                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - M√≥dulo: load_app/data_loading_unidades_proyecto.py   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Actualizaciones selectivas por upid                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Comparaci√≥n de campos (solo actualiza si cambi√≥)     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Firebase Firestore                            ‚îÇ
‚îÇ              Colecci√≥n: unidades_proyecto                       ‚îÇ
‚îÇ           Identificador √∫nico: upid (UNP-XXXX)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Prerrequisitos

### Software Requerido

```bash
# Google Cloud SDK
gcloud --version  # M√≠nimo: 450.0.0

# Python
python --version  # M√≠nimo: 3.11

# Git
git --version
```

### Permisos Necesarios en GCP

El Service Account o usuario debe tener:

- ‚úÖ `Cloud Functions Developer`
- ‚úÖ `Cloud Scheduler Admin`
- ‚úÖ `Secret Manager Secret Accessor`
- ‚úÖ `Firebase Admin`
- ‚úÖ `Service Account User`

---

## üîê Configuraci√≥n de Credenciales

### 1. Service Account para Firebase

```bash
# Crear Service Account
gcloud iam service-accounts create etl-pipeline-sa \
    --display-name="ETL Pipeline Service Account" \
    --project=YOUR_PROJECT_ID

# Asignar roles
gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
    --member="serviceAccount:etl-pipeline-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/firebase.admin"

gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
    --member="serviceAccount:etl-pipeline-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/datastore.user"

# Descargar clave (solo para desarrollo local)
gcloud iam service-accounts keys create firebase-service-account.json \
    --iam-account=etl-pipeline-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com
```

### 2. Configurar Acceso a Google Drive

**Opci√≥n A: Usar la misma Service Account**

1. Ir a Google Drive
2. Compartir la carpeta con el email de la Service Account
3. Dar permisos de **Viewer**

**Opci√≥n B: Usar credenciales de usuario (OAuth)**

```bash
gcloud auth application-default login \
    --scopes=https://www.googleapis.com/auth/drive.readonly
```

### 3. Guardar Credenciales en Secret Manager (Producci√≥n)

```bash
# Crear secret para Firebase
gcloud secrets create firebase-credentials \
    --data-file=firebase-service-account.json \
    --replication-policy="automatic" \
    --project=YOUR_PROJECT_ID

# Dar acceso a la Cloud Function
gcloud secrets add-iam-policy-binding firebase-credentials \
    --member="serviceAccount:etl-pipeline-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/secretmanager.secretAccessor"
```

---

## üöÄ Despliegue de Cloud Function

### 1. Preparar el C√≥digo

```bash
cd cloud_functions

# Verificar archivos necesarios
ls -la
# Debe contener:
# - main.py
# - requirements.txt
# - utils.py (opcional si usas S3)
```

### 2. Desplegar Cloud Function (Gen 2)

```bash
gcloud functions deploy etl-pipeline-hourly \
    --gen2 \
    --runtime=python311 \
    --region=us-central1 \
    --source=. \
    --entry-point=etl_pipeline_hourly \
    --trigger-http \
    --allow-unauthenticated \
    --memory=2048MB \
    --timeout=540s \
    --max-instances=1 \
    --service-account=etl-pipeline-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com \
    --set-env-vars=FIREBASE_PROJECT_ID=YOUR_PROJECT_ID,SERVICE_ACCOUNT_FILE=/tmp/firebase-creds.json \
    --project=YOUR_PROJECT_ID
```

**Par√°metros Importantes:**

- `--memory=2048MB`: Memoria suficiente para procesamiento geoespacial
- `--timeout=540s`: 9 minutos (m√°ximo para Cloud Functions)
- `--max-instances=1`: Evita ejecuciones paralelas no deseadas
- `--gen2`: Usa Cloud Functions Gen 2 (m√°s potente)

### 3. Verificar Despliegue

```bash
# Listar funciones
gcloud functions list --project=YOUR_PROJECT_ID

# Obtener URL de la funci√≥n
gcloud functions describe etl-pipeline-hourly \
    --region=us-central1 \
    --gen2 \
    --format="value(serviceConfig.uri)" \
    --project=YOUR_PROJECT_ID
```

---

## ‚è∞ Configuraci√≥n de Cloud Scheduler

### 1. Crear Job de Scheduler

```bash
# Crear job que se ejecuta cada hora desde medianoche
gcloud scheduler jobs create http etl-pipeline-hourly-job \
    --location=us-central1 \
    --schedule="0 * * * *" \
    --time-zone="America/Bogota" \
    --uri="https://REGION-PROJECT_ID.cloudfunctions.net/etl-pipeline-hourly" \
    --http-method=POST \
    --oidc-service-account-email=etl-pipeline-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com \
    --project=YOUR_PROJECT_ID
```

**Cron Schedule Explicado:**

- `0 * * * *` = Cada hora a los 0 minutos (00:00, 01:00, 02:00, ...)
- Comienza desde medianoche (00:00)
- Se ejecuta 24 veces al d√≠a

**Otras opciones de schedule:**

```bash
# Cada 2 horas
"0 */2 * * *"

# Solo durante horas laborales (8 AM - 6 PM)
"0 8-18 * * *"

# Cada 30 minutos
"*/30 * * * *"
```

### 2. Verificar Scheduler

```bash
# Listar jobs
gcloud scheduler jobs list --location=us-central1 --project=YOUR_PROJECT_ID

# Ver detalles del job
gcloud scheduler jobs describe etl-pipeline-hourly-job \
    --location=us-central1 \
    --project=YOUR_PROJECT_ID

# Ejecutar manualmente (testing)
gcloud scheduler jobs run etl-pipeline-hourly-job \
    --location=us-central1 \
    --project=YOUR_PROJECT_ID
```

---

## üîß Variables de Entorno

### En Cloud Function

Configurar en `gcloud functions deploy`:

```bash
--set-env-vars="\
FIREBASE_PROJECT_ID=YOUR_PROJECT_ID,\
SERVICE_ACCOUNT_FILE=/tmp/firebase-creds.json,\
DRIVE_UNIDADES_PROYECTO_FOLDER_ID=YOUR_DRIVE_FOLDER_ID,\
FIRESTORE_BATCH_SIZE=100,\
FIRESTORE_TIMEOUT=30"
```

### En Desarrollo Local

Crear archivo `.env.local`:

```bash
# Firebase
FIREBASE_PROJECT_ID=your-project-id

# Google Drive
DRIVE_UNIDADES_PROYECTO_FOLDER_ID=your-folder-id
SERVICE_ACCOUNT_FILE=firebase-service-account.json

# Firestore
FIRESTORE_BATCH_SIZE=100
FIRESTORE_TIMEOUT=30
```

---

## ‚úÖ Testing y Validaci√≥n

### 1. Test Local

```bash
# Activar entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate  # Windows

# Instalar dependencias
pip install -r cloud_functions/requirements.txt

# Ejecutar funci√≥n localmente
cd cloud_functions
functions-framework --target=etl_pipeline_hourly --debug
```

Hacer request HTTP:

```bash
curl -X POST http://localhost:8080
```

### 2. Test en Cloud

```bash
# Invocar Cloud Function directamente
gcloud functions call etl-pipeline-hourly \
    --region=us-central1 \
    --gen2 \
    --project=YOUR_PROJECT_ID

# Ejecutar via Scheduler
gcloud scheduler jobs run etl-pipeline-hourly-job \
    --location=us-central1 \
    --project=YOUR_PROJECT_ID
```

### 3. Validar Carga en Firebase

```bash
# Contar documentos (usando gcloud firestore)
gcloud firestore operations list --project=YOUR_PROJECT_ID

# O desde Python
python -c "
from database.config import get_firestore_client
db = get_firestore_client()
docs = list(db.collection('unidades_proyecto').stream())
print(f'Total documentos: {len(docs)}')
"
```

---

## üìä Monitoreo y Logs

### Ver Logs de Cloud Function

```bash
# Logs en tiempo real
gcloud functions logs read etl-pipeline-hourly \
    --region=us-central1 \
    --gen2 \
    --limit=50 \
    --project=YOUR_PROJECT_ID

# Logs con filtros
gcloud logging read "resource.type=cloud_function
    AND resource.labels.function_name=etl-pipeline-hourly
    AND severity>=ERROR" \
    --limit=50 \
    --format=json \
    --project=YOUR_PROJECT_ID
```

### Dashboard en GCP Console

1. Ir a **Cloud Functions** ‚Üí `etl-pipeline-hourly`
2. Tab **Logs**
3. Filtrar por:
   - Severity: Error, Warning
   - Timestamp: √öltimas 24 horas

### M√©tricas Clave

Monitorear:

- ‚úÖ **Invocations**: Debe ser ~24/d√≠a (cada hora)
- ‚úÖ **Execution time**: Debe ser < 540s
- ‚úÖ **Errors**: Debe ser 0
- ‚úÖ **Memory usage**: No debe exceder 2GB

---

## üîß Troubleshooting

### Error: "Service Account doesn't have permission"

```bash
# Verificar roles
gcloud projects get-iam-policy YOUR_PROJECT_ID \
    --flatten="bindings[].members" \
    --filter="bindings.members:etl-pipeline-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com"

# Agregar roles faltantes
gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
    --member="serviceAccount:etl-pipeline-sa@YOUR_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/cloudfunctions.invoker"
```

### Error: "Module not found" en Cloud Function

**Causa**: Dependencias faltantes en `requirements.txt`

**Soluci√≥n**:

```bash
# Generar requirements.txt actualizado
pip freeze > requirements.txt

# Re-desplegar
gcloud functions deploy etl-pipeline-hourly ...
```

### Error: "Timeout exceeded"

**Causa**: Procesamiento tarda > 540s

**Soluciones**:

1. **Aumentar memoria** (m√°s CPU):

   ```bash
   --memory=4096MB
   ```

2. **Optimizar c√≥digo**:

   - Reducir batch size
   - Procesar en paralelo
   - Usar √≠ndices en Firebase

3. **Dividir en m√∫ltiples funciones**:
   - Funci√≥n 1: Extracci√≥n + Transformaci√≥n
   - Funci√≥n 2: Carga a Firebase

### Error: "RESOURCE_EXHAUSTED" en Firestore

**Causa**: L√≠mites de escritura excedidos

**Soluci√≥n**:

```python
# En load_app/data_loading_unidades_proyecto.py
# Aumentar delay entre batches
time.sleep(0.5)  # Aumentar de 0.1 a 0.5
```

### Scheduler no ejecuta la funci√≥n

```bash
# Verificar status del job
gcloud scheduler jobs describe etl-pipeline-hourly-job \
    --location=us-central1 \
    --project=YOUR_PROJECT_ID

# Ver √∫ltimas ejecuciones
gcloud scheduler jobs list --location=us-central1 --project=YOUR_PROJECT_ID

# Habilitar job si est√° pausado
gcloud scheduler jobs resume etl-pipeline-hourly-job \
    --location=us-central1 \
    --project=YOUR_PROJECT_ID
```

---

## üìö Recursos Adicionales

### Documentaci√≥n Oficial

- [Cloud Functions Gen 2](https://cloud.google.com/functions/docs/2nd-gen/overview)
- [Cloud Scheduler](https://cloud.google.com/scheduler/docs)
- [Firebase Admin SDK](https://firebase.google.com/docs/admin/setup)
- [Secret Manager](https://cloud.google.com/secret-manager/docs)

### Comandos √ötiles

```bash
# Pausar scheduler
gcloud scheduler jobs pause etl-pipeline-hourly-job \
    --location=us-central1 \
    --project=YOUR_PROJECT_ID

# Reanudar scheduler
gcloud scheduler jobs resume etl-pipeline-hourly-job \
    --location=us-central1 \
    --project=YOUR_PROJECT_ID

# Eliminar Cloud Function
gcloud functions delete etl-pipeline-hourly \
    --region=us-central1 \
    --gen2 \
    --project=YOUR_PROJECT_ID

# Eliminar Scheduler job
gcloud scheduler jobs delete etl-pipeline-hourly-job \
    --location=us-central1 \
    --project=YOUR_PROJECT_ID
```

---

## üéâ Conclusi√≥n

Has configurado exitosamente un pipeline ETL serverless que:

‚úÖ Se ejecuta autom√°ticamente cada hora  
‚úÖ Procesa datos desde Google Drive  
‚úÖ Actualiza Firebase de forma selectiva  
‚úÖ No expone credenciales  
‚úÖ Es 100% serverless (sin servidores que mantener)

Para soporte o preguntas, consulta los logs o revisa la documentaci√≥n oficial de GCP.
