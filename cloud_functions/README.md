# Cloud Functions - Pipeline ETL Serverless Completo

## ðŸ“‹ DescripciÃ³n

Sistema serverless 100% en Google Cloud Platform que ejecuta **cada hora** el pipeline ETL completo:

- âœ… **Extrae** datos desde Google Drive
- âœ… **Transforma** datos (procesamiento geoespacial, validaciÃ³n, normalizaciÃ³n)
- âœ… **Carga** a Firebase Firestore con actualizaciones selectivas por `upid`
- âœ… Solo actualiza campos que han cambiado (no todo siempre)
- âœ… Sin credenciales expuestas (usa Secret Manager)

## ðŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Cloud Scheduler                              â”‚
â”‚                   Cron: 0 * * * * (cada hora)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP Trigger
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Cloud Function: etl_pipeline_hourly                â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ETAPA 1: ExtracciÃ³n (Google Drive)                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ETAPA 2: TransformaciÃ³n                                 â”‚  â”‚
â”‚  â”‚  - Procesamiento geoespacial                             â”‚  â”‚
â”‚  â”‚  - ValidaciÃ³n de coordenadas                             â”‚  â”‚
â”‚  â”‚  - NormalizaciÃ³n de datos                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ETAPA 3: Carga a Firebase                               â”‚  â”‚
â”‚  â”‚  - Actualizaciones selectivas por upid                   â”‚  â”‚
â”‚  â”‚  - Solo actualiza si cambiÃ³                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Firebase Firestore                            â”‚
â”‚              ColecciÃ³n: unidades_proyecto                       â”‚
â”‚           Identificador Ãºnico: upid (UNP-XXXX)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Inicio RÃ¡pido (5 minutos)

### 1. Desplegar Cloud Function

**Linux/Mac:**

```bash
cd cloud_functions
chmod +x deploy-cloud-function.sh
./deploy-cloud-function.sh YOUR_PROJECT_ID us-central1
```

**Windows:**

```powershell
cd cloud_functions
.\deploy-cloud-function.ps1 -ProjectId "YOUR_PROJECT_ID" -Region "us-central1"
```

### 2. Configurar EjecuciÃ³n AutomÃ¡tica (cada hora)

**Linux/Mac:**

```bash
chmod +x setup-cloud-scheduler.sh
./setup-cloud-scheduler.sh YOUR_PROJECT_ID us-central1
```

**Windows:**

```powershell
.\setup-cloud-scheduler.ps1 -ProjectId "YOUR_PROJECT_ID" -Region "us-central1"
```

### 3. Verificar

```bash
# Ver logs
gcloud functions logs read etl-pipeline-hourly \
    --region=us-central1 \
    --limit=50

# Ejecutar manualmente (testing)
gcloud scheduler jobs run etl-pipeline-hourly-job \
    --location=us-central1
```

## ðŸ“š DocumentaciÃ³n Completa

- **[QUICK_START.md](QUICK_START.md)** - GuÃ­a de inicio rÃ¡pido âš¡
- **[SERVERLESS_DEPLOYMENT_GUIDE.md](SERVERLESS_DEPLOYMENT_GUIDE.md)** - GuÃ­a completa y detallada ðŸ“–

## ðŸ“ Mapeo de Campos

### Unidades de Proyecto

| Campo Origen             | Campo Destino          | CondiciÃ³n                         |
| ------------------------ | ---------------------- | --------------------------------- |
| `comuna_corregimiento_2` | `comuna_corregimiento` | `fuera_rango == 'ACEPTABLE'`      |
| `comuna_corregimiento`   | `comuna_corregimiento` | `fuera_rango == 'FUERA DE RANGO'` |
| `barrio_vereda_2`        | `barrio_vereda`        | Si `barrio_vereda_2` es vÃ¡lido    |
| `barrio_vereda`          | `barrio_vereda`        | Fallback                          |
| `fecha_inicio_std`       | `fecha_inicio`         | Siempre                           |
| `fecha_fin_std`          | `fecha_fin`            | Siempre                           |
| `upid`                   | `upid`                 | Identificador Ãºnico               |

### GeometrÃ­a

La geometrÃ­a GeoJSON se convierte a formato Firestore:

- **Point**: `GeoPoint(longitude, latitude)`
- **Polygon**: `{type: 'polygon', coordinates: [[[...]]]}`
- **MultiPolygon**: `{type: 'multipolygon', coordinates: [[[[...]]]]}`

## ðŸ”§ Uso

### EjecuciÃ³n Manual

```powershell
# Usando Invoke-WebRequest
$url = "https://REGION-PROJECT_ID.cloudfunctions.net/manual-trigger-unidades-proyecto"
Invoke-WebRequest -Uri $url -Method POST

# Con curl
curl -X POST https://REGION-PROJECT_ID.cloudfunctions.net/manual-trigger-unidades-proyecto
```

### EjecuciÃ³n desde Pipeline Python

```python
# En pipelines/unidades_proyecto_pipeline.py
import requests

def trigger_cloud_function():
    url = "https://REGION-PROJECT_ID.cloudfunctions.net/load-unidades-proyecto"
    response = requests.post(url)

    if response.status_code == 200:
        result = response.json()
        print(f"Nuevos: {result['stats']['unidades_proyecto']['new']}")
        print(f"Actualizados: {result['stats']['unidades_proyecto']['updated']}")
        print(f"Sin cambios: {result['stats']['unidades_proyecto']['unchanged']}")
    else:
        print(f"Error: {response.status_code}")
        print(response.text)
```

### EjecuciÃ³n AutomÃ¡tica

Cloud Scheduler ejecuta diariamente a las 2:00 AM (si fue configurado):

```powershell
# Ver jobs de scheduler
gcloud scheduler jobs list

# Ejecutar manualmente un job
gcloud scheduler jobs run etl-unidades-proyecto-daily --location=us-central1

# Pausar/reanudar
gcloud scheduler jobs pause etl-unidades-proyecto-daily --location=us-central1
gcloud scheduler jobs resume etl-unidades-proyecto-daily --location=us-central1
```

## ðŸ“Š Monitoreo

### Logs

```powershell
# Ver logs en tiempo real
gcloud functions logs read load-unidades-proyecto --region=us-central1 --limit=50

# Filtrar errores
gcloud functions logs read load-unidades-proyecto --region=us-central1 | Select-String "ERROR"
```

### MÃ©tricas

Ver en GCP Console:

- **Cloud Functions â†’ load-unidades-proyecto â†’ Metrics**
  - Invocaciones
  - Tiempo de ejecuciÃ³n
  - Errores
  - Uso de memoria

### Firestore

Verificar colecciones:

```powershell
# Con gcloud
gcloud firestore collections list

# Contar documentos (aproximado)
gcloud firestore operations list
```

## ðŸ” Seguridad

### Credenciales AWS

Almacenadas en **Secret Manager** (no en cÃ³digo):

- Secret: `aws-credentials`
- Formato: JSON con `aws_access_key_id`, `aws_secret_access_key`, `region`
- Acceso: Solo Service Account de Cloud Functions

### Service Account

Permisos mÃ­nimos necesarios:

- `roles/secretmanager.secretAccessor`: Leer credenciales AWS
- `roles/datastore.user`: Escribir a Firestore

### AutenticaciÃ³n HTTP

Actualmente: `--allow-unauthenticated` para facilitar trigger manual.

**Para producciÃ³n**, cambiar a autenticado:

```powershell
gcloud functions deploy load-unidades-proyecto \
  --no-allow-unauthenticated \
  ...
```

Y usar tokens de autenticaciÃ³n:

```powershell
$token = gcloud auth print-identity-token
Invoke-WebRequest -Uri $url -Method POST -Headers @{"Authorization"="Bearer $token"}
```

## ðŸ› Troubleshooting

### Error: "Secret not found"

```powershell
# Verificar secrets
gcloud secrets list

# Re-crear secret
.\setup_cloud_functions.ps1
```

### Error: "Permission denied on Firestore"

```powershell
# Verificar permisos de Service Account
gcloud projects get-iam-policy YOUR_PROJECT_ID \
  --flatten="bindings[].members" \
  --filter="bindings.members:serviceAccount:cloud-functions-etl@*"

# Re-asignar permisos
gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
  --member="serviceAccount:cloud-functions-etl@YOUR_PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/datastore.user"
```

### Error: "Cannot connect to S3"

- Verificar que `aws_credentials.json` tenga credenciales vÃ¡lidas
- Verificar que Secret Manager tenga la Ãºltima versiÃ³n
- Verificar permisos IAM de usuario AWS en bucket S3

### Timeout en ejecuciÃ³n

Si procesa muchos datos (>1000 registros):

```powershell
# Aumentar timeout a 9 minutos
gcloud functions deploy load-unidades-proyecto \
  --timeout=540s \
  --memory=1GB \
  ...
```

## ðŸ“ˆ Optimizaciones

### Batch Size

Actualmente: 500 documentos por batch.

Para ajustar, editar `cloud_functions/utils.py`:

```python
# FirestoreHandler.batch_upsert()
BATCH_SIZE = 500  # Cambiar segÃºn necesidad
```

### Cache de Documentos Existentes

Evita lecturas innecesarias guardando hashes en memoria durante ejecuciÃ³n.

### ComparaciÃ³n MD5

Solo escribe a Firestore si el hash MD5 del documento cambiÃ³, ahorrando:

- Write operations (costo)
- Triggers innecesarios
- Bandwidth

## ðŸ”„ ActualizaciÃ³n

Para actualizar funciones despuÃ©s de cambios en cÃ³digo:

```powershell
# Re-deploy automÃ¡tico
.\setup_cloud_functions.ps1

# O manual
cd cloud_functions
gcloud functions deploy load-unidades-proyecto \
  --gen2 \
  --runtime=python311 \
  --region=us-central1 \
  --source=. \
  --entry-point=load_unidades_proyecto_from_s3
```

## ðŸ“ž Soporte

- **Logs**: Cloud Functions Console â†’ Logs
- **Errores**: Ver secciÃ³n Troubleshooting
- **Performance**: Metrics tab en Cloud Functions Console

## ðŸŽ¯ PrÃ³ximos Pasos

1. âœ… Deploy inicial con `setup_cloud_functions.ps1`
2. âœ… Probar ejecuciÃ³n manual
3. âœ… Verificar datos en Firestore
4. â¬œ Configurar alertas en Cloud Monitoring
5. â¬œ Implementar Cloud Logging para anÃ¡lisis avanzado
6. â¬œ Considerar Cloud Run para workloads mÃ¡s pesados
